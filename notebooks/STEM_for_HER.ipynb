{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vibe Check\n",
    "### Using Twitter’s API and Sentiment Analysis to Understand What’s the What on the Internet\n",
    "\n",
    "Today's session will cover:\n",
    "1.   Setting up Access to the Twitter API and Getting API Access Keys\n",
    "2.   Getting Data from Twitter using the Twitter API\n",
    "3.   Basic Data Operations and Data Cleaning\n",
    "4.   Sentiment Analysis With Python using NLTK\n",
    "\n",
    "<!--- TODO: Slides with: Intros - who we are, what does FN do overview, session goals/ what is an API + QR code with link to public google colab + session end slides - career guidance? (One of our learning objectives was \"What jobs or internships can you search for to use the skills covered in this workshop?\" -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhEkeNWy6Jp5"
   },
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs\n",
    "### What are APIs?\n",
    "\n",
    "An API is the most popular way to access data programmatically - API documentation will tell our clients what is available and how to “ask” our API for it.\n",
    "\n",
    "If you've ever seen tweets embedded on a webpage, those were pulled in via an API!\n",
    "\n",
    "First, we're going to set up some libraries, and our API authentication information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our \"bearer_token\" - like a secret password that belongs to only us so Twitter knows who exactly is asking it for data - stored in a file. We're going to read the data in:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll set up what we need to make the actual request to the Twitter API:\n",
    "1. The information telling Twitter exactly who we are:\n",
    "    - **`bearer_token`**: the secret password, to Authenticate us\n",
    "    - **`User-Agent`**: a name for what project we're working on.\n",
    "    - This information is important for Twitter to track so they can keep track of who is using their API and make sure that nobody is abusing the API. Pretty much every API will require you to identify yourself in some way before you can get data back.\n",
    "\n",
    "\n",
    "2. The URL we're going to request. In this case: `https://api.twitter.com/2/tweets/search/recent`\n",
    "    - **`api.twitter.com`**: tells Twitter we're trying to hit the API, as opposed to the main feed/user interface.\n",
    "    - **`2`**: shows that we're hitting Version 2.0 of the API. If we put `1` instead, we would hit the 1st version, which would both require slightly different request syntax, and would return data formatted differently.\n",
    "    - **`tweets`**: indicates which data type we want to request. We could also input `users`, `spaces`, or `lists` to get different datatypes back.\n",
    "    - **`search`**: says we want to search over tweets. We could also put `counts` to get the number of tweets, or we could look up tweets directly by their IDs. `search` allows us to give Twitter a query - a set of terms we want to include or exclude - and we'll get back tweets that match our query terms.\n",
    "    - **`recent`**: Twitter allows you to search either over only Tweets from the last week, or `all` Tweets, depending on your level of access. We'll stick to `recent`, because we're interested in what's happening on Twitter right now. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add comments explaining things\n",
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "with open(f\"../utils/bearer_token.txt\", \"r\") as token_file:\n",
    "    bearer_token = token_file.read()\n",
    "    \n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "    \"User-Agent\": \"stem-for-her-demo\"\n",
    "}\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent?\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building a Query\n",
    "\n",
    "See: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "\n",
    "<!--- Audience Participation here - ask for hashtags/ keyword search ideas - maybe pull up twitter trends on a screen? Live edit notebook to change search keywords-->\n",
    "\n",
    "-- Harry Styles, Elon Musk, other things that are trending, Taylor Swift tour?\n",
    "\n",
    "### Optional Fields\n",
    "tweet.fields lets us add specific fields -  here we add `created_at`\n",
    "\n",
    "### Query String\n",
    "\n",
    "-is:retweet *excludes* any retweets\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_string = '#twitter ' # tweets #HarryStyles hashtag\n",
    "query_string += '\"elon musk\" ' # tweets that have \"watermelon sugar\" somewhere in their text\n",
    "query_string += '-is:retweet ' # eliminate retweets\n",
    "print(query_string)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "See all the different operators types you can add to your search here: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#operators"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GET vs POST requests\n",
    "\n",
    "When using APIs, there are multiple ways you can engage with them. The API Documentation will tell you what you're able to do, but one important thing to know about is what _type_ of requests you can make.\n",
    "\n",
    "`GET` requests are exactly what they sound like - you usually use them to _GET_ data back from the API. \n",
    "`POST` requests are a little more complicated, but generally they are used to _create_ data via the API. Any Twitter Bot you see is going to be using POST requests to create Tweets. See: https://twitter.com/MagicRealismBot\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_widget = widgets.Textarea(\n",
    "    value= query_string,\n",
    "    placeholder='Enter a search string',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")\n",
    "display(query_widget)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HTTP Status Codes\n",
    ">> Karnika TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_params = {'query': query_widget.value,\n",
    "                'tweet.fields': 'created_at,id,lang,source,text', # what data we want to return\n",
    "                'expansions': 'author_id'     # will include the profile ID of the author\n",
    "               }\n",
    "\n",
    "response = requests.get(url=search_url,params=query_params,\n",
    "                       headers=headers)\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What is JSON"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_request = \"https://api.twitter.com/2/tweets/counts/recent?query=\" + urllib.parse.quote(query_widget.value) + \"&granularity=day\"\n",
    "tweet_counts = requests.get(count_request, headers=headers)\n",
    "print(tweet_counts.status_code)\n",
    "print(json.dumps(tweet_counts.json(), indent=2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizing things"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#%pip install plotly\n",
    "#%pip install pandas\n",
    "# create a requirements.txt file\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "tweet_count_df = pd.DataFrame(tweet_counts.json()[\"data\"])\n",
    "tweet_count_df.head(5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# When did dates get so complicated\n",
    "#alwaysbegoogling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "for i, r in tweet_count_df.iterrows():\n",
    "    tweet_count_df.loc[i, 'day'] = dt.strptime(tweet_count_df.loc[i, 'start'], \"%Y-%m-%dT%H:%M:%S.%fZ\").date()\n",
    "    \n",
    "tweet_count_df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x=tweet_count_df['day'].astype(dtype=str), \n",
    "                        y=tweet_count_df['tweet_count'],\n",
    "                        marker_color='indianred', text=\"tweet_count\"))\n",
    "fig.update_layout({\"title\": 'Recent tweets',\n",
    "                   \"xaxis\": {\"title\":\"Days\"},\n",
    "                   \"yaxis\": {\"title\":\"Total tweets\"},\n",
    "                   \"showlegend\": False})\n",
    "#fig.write_image(\"by-day.png\",format=\"png\", width=1000, height=600, scale=3)\n",
    "fig.show()\n",
    "# Add query to chart so we know what the search was?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create widget again"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting Started with Sentiment Analysis\n",
    "\n",
    "<!--- TODO: Add more here - what is NLP? Short explanation of word tokenization\n",
    "Ref: https://realpython.com/python-nltk-sentiment-analysis/#using-nltks-pre-trained-sentiment-analyzer\n",
    "-->"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#%pip install nltk\n",
    "import nltk\n",
    "\n",
    "#nltk.download([\"names\", \"stopwords\", \"averaged_perceptron_tagger\", \"vader_lexicon\",\"punkt\"])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words = []\n",
    "json_response = response.json()\n",
    "for item in json_response.get('data'):\n",
    "    words.extend(nltk.word_tokenize(item.get('text')))\n",
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "words_clean = [w for w in words if w.isalpha() and w not in unwanted]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(words_clean)\n",
    "print(fd.most_common(10))\n",
    "print(fd.tabulate(5))\n",
    "# Remove the https? is that in stopwords?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What is NLP, Machine Learning etc\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "tweets = [t['text'].replace(\"://\", \"//\") for t in json_response.get('data')]\n",
    "\n",
    "def is_positive(tweet: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0, sia.polarity_scores(tweet)[\"compound\"]\n",
    "\n",
    "compound_sentiment = {'tweet': [], 'pos': [], 'comp_score': []}\n",
    "\n",
    "for t in tweets:\n",
    "    compound_sentiment['tweet'].append(t)\n",
    "    is_pos, score = is_positive(t)\n",
    "    compound_sentiment['pos'].append(is_pos)\n",
    "    compound_sentiment['comp_score'].append(score)\n",
    "\n",
    "compound_sentiment_df = pd.DataFrame(compound_sentiment)\n",
    "compound_sentiment_df.head(5)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_gauge():\n",
    "    fig_gc = go.Figure(go.Indicator(\n",
    "    mode = \"gauge+number\",\n",
    "    value = tw_select.value,\n",
    "    gauge = {'axis': {'range': [-1, 1]}, \n",
    "             'bar': {'color':'darkslategray'},\n",
    "             'steps': [{'range': [-1, 0], 'color': 'lightcoral'}, {'range': [0, 1], 'color': 'lightgreen'}]},\n",
    "    domain = {'x': [0,1], 'y': [0,1]},\n",
    "    title = \"Average sentiment\"))\n",
    "    # make space for explanation / annotation\n",
    "    fig_gc.update_layout(margin=dict(l=20, r=20, t=20, b=60),paper_bgcolor=\"white\")\n",
    "\n",
    "    # add annotation\n",
    "    fig_gc.add_annotation(dict(font=dict(color='darkslategray',size=15),\n",
    "                                        x=0,\n",
    "                                        y=-0.12,\n",
    "                                        showarrow=False,\n",
    "                                        text=tw_select.label,\n",
    "                                        textangle=0,\n",
    "                                        xanchor='left',\n",
    "                                        xref=\"paper\",\n",
    "                                        yref=\"paper\"))\n",
    "    fig_gc.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_sentiment = compound_sentiment_df.mean(numeric_only=True)['comp_score']\n",
    "\n",
    "tw_options = [(compound_sentiment_df.loc[i, 'tweet'], compound_sentiment_df.loc[i, 'comp_score']) for i, r in compound_sentiment_df.iterrows()]\n",
    "tw_options.append(('Average Score', mean_sentiment))\n",
    "tw_select = widgets.Dropdown(options=tw_options,\n",
    "                             value = mean_sentiment,\n",
    "                             description='Tweet:')\n",
    "caption = widgets.Label(value='initial pos')\n",
    "\n",
    "def handle_change(change):\n",
    "    create_gauge()\n",
    "    \n",
    "\n",
    "tw_select.observe(handle_change)\n",
    "\n",
    "display(tw_select)\n",
    "\n",
    "create_gauge()\n",
    "# Add garbage collection for gauge\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
